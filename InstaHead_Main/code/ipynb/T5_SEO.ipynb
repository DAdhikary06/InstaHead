{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WTWDaIhTDlVw",
        "5TUgBHuLgNH-",
        "I35Y_D3SewYy",
        "vOn13LJXaVqZ",
        "2BNJKaob-LRc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model Load"
      ],
      "metadata": {
        "id": "VFi0-3tpLKPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_path = \"/content/drive/MyDrive/T5_e10a5\""
      ],
      "metadata": {
        "id": "2IbtkV8OLPTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/T5_Headline_Model\""
      ],
      "metadata": {
        "id": "aXJVcsj7CvoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer"
      ],
      "metadata": {
        "id": "oaiuTTP9LTgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "Ge5HP4soLYZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "U5V4BJT9rDhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9kol3mYqrPPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths = [\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-2.csv',\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-3.csv',\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-4.csv',\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-5.csv',\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-6.csv',\n",
        "    '/content/drive/MyDrive/Inshort-News-DataSet/inshort_news_data-7.csv',\n",
        "  ]\n",
        "dataframes = [pd.read_csv(path) for path in paths] # returns List\n",
        "data = pd.concat(dataframes)\n",
        "df = data.sample(frac=1).reset_index(drop=True) # After 100% sampling data"
      ],
      "metadata": {
        "id": "Sny3PYJ2q7Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]['news_article']"
      ],
      "metadata": {
        "id": "B-TTeqhsvhxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6b3a815d-a43f-4630-b548-9974288fd3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla was ordered by a Chinese court to pay over â‚¹1 crore to the buyer of a used Model S car after concluding it concealed structural damage on the vehicle it sold on its official website. It was reportedly discovered part of the vehicle had been cut and welded back together. Tesla will appeal the ruling to a higher court.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]['news_headline']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5eWcESVDFYy7",
        "outputId": "b400d04d-879b-46cb-e410-cf9f49c63d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla asked to give â‚¹1 cr to used car buyer for hiding damage in China'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_headline = \"Tesla asked to give â‚¹1 cr to used car buyer for hiding damage in China\""
      ],
      "metadata": {
        "id": "-uF3e5FqOgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Headline Generators"
      ],
      "metadata": {
        "id": "AKhSDWqRA_mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LogitsProcessor\n",
        "from typing import Dict"
      ],
      "metadata": {
        "id": "PXIVYaRoFjYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Keyword Extraction (manual/simple for demo)\n",
        "article = \"\"\"Tesla was ordered by a Chinese court to pay over â‚¹1 crore to the buyer of a used Model S car after concluding it concealed structural damage on the vehicle it sold on its official website. It was reportedly discovered part of the vehicle had been cut and welded back together. Tesla will appeal the ruling to a higher court.\"\"\"\n",
        "# article = df.iloc[0]['news_article']\n",
        "keyword = \"Chinese\"  # manually extracted for this demo"
      ],
      "metadata": {
        "id": "LqPT6QR4GBGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Prepare input\n",
        "input_text = \"summarize: \" + article\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)"
      ],
      "metadata": {
        "id": "Kczulie5G1wX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updated LogitsProcessor with Sampling instead of Beam Search  \n",
        "Status: **Functional**  \n",
        "Conclusion: Working for single token.  "
      ],
      "metadata": {
        "id": "G07gdbjRLdmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create scores_map with token ID(s) of the keyword ===\n",
        "keyword_ids = tokenizer.encode(keyword, add_special_tokens=False)\n",
        "scores_map = {token_id: 8.0 for token_id in keyword_ids}  # Strong additive bias"
      ],
      "metadata": {
        "id": "ciR4qxCqB1sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define Additive SEOLogitsProcessor ===\n",
        "class SEOLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, scores_map: Dict[int, float]):\n",
        "        self.scores_map = scores_map\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        for token_id, boost in self.scores_map.items():\n",
        "            scores[:, token_id] += boost\n",
        "        return scores"
      ],
      "metadata": {
        "id": "ozDx2jvOCDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Generate WITHOUT SEO biasing (baseline) ===\n",
        "seo_processor = SEOLogitsProcessor(scores_map)\n",
        "output_ids_plain = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.9\n",
        ")\n",
        "title_plain = tokenizer.decode(output_ids_plain[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "7MCkh5CTCKbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate WITH SEO biasing ===\n",
        "output_ids_seo = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.6,\n",
        "    logits_processor=[seo_processor]\n",
        ")\n",
        "title_seo = tokenizer.decode(output_ids_seo[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "AayDxaxDKOvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. Show Results ===\n",
        "print(\"ðŸ”¹ Without SEO Biasing:\", title_plain)\n",
        "print(\"ðŸ”¹ With SEO Biasing   :\", title_seo)\n",
        "print(\"ðŸ”¹ Keyword Biased Toward:\", keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhICYtbWCOrP",
        "outputId": "f844f473-1023-423c-d776-62e11f842497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Without SEO Biasing: Tesla ordered to pay over 1 cr to used car buyer for hiding damage in China\n",
            "ðŸ”¹ With SEO Biasing   : Tesla ordered to pay 1 cr to Chinese buyer for concealing damage to Model S\n",
            "ðŸ”¹ Keyword Biased Toward: Chinese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logits Processor  \n",
        "\n",
        "Conclusion: The biasing does not influence the results as beam search is being used."
      ],
      "metadata": {
        "id": "WTWDaIhTDlVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Get keyword token ID and create scores_map\n",
        "keyword_ids = tokenizer.encode(keyword, add_special_tokens=False)\n",
        "scores_map = {k: 1.0 for k in keyword_ids}  # 1.0 is arbitrary; can experiment"
      ],
      "metadata": {
        "id": "15ajKdUEGlJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define SEOLogitsProcessor\n",
        "class SEOLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, scores_map: Dict[int, float], temperature: float, vocab_size: int):\n",
        "        self.temperature = temperature\n",
        "        self.mask = torch.ones(vocab_size)\n",
        "        self.seo_words_ids = list(scores_map.keys())\n",
        "        for k, v in scores_map.items():\n",
        "            v = max(v, 0.0001)\n",
        "            self.mask[k] = (10 / v) * temperature\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        if self.temperature == 1:\n",
        "            return scores\n",
        "        for k in self.seo_words_ids:\n",
        "            self.mask[k] *= 1.1\n",
        "        return scores * self.mask.to(scores.device)"
      ],
      "metadata": {
        "id": "h9LEpkkBGqKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate without SEO processor\n",
        "output_ids_plain = model.generate(input_ids, max_length=20, num_beams=4)\n",
        "title_plain = tokenizer.decode(output_ids_plain[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "AK7YPc4WIOeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Generate with SEO processor\n",
        "seo_processor = SEOLogitsProcessor(scores_map, temperature=0.9, vocab_size=model.config.vocab_size)\n",
        "output_ids_seo = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    num_beams=4,\n",
        "    logits_processor=[seo_processor]\n",
        ")\n",
        "title_seo = tokenizer.decode(output_ids_seo[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "_mPo6cAZISmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Print results\n",
        "print(\"ðŸ”¹Original Title: \", title_plain)\n",
        "print(\"ðŸ”¹SEO-Biased Title: \", title_seo)\n",
        "print(\"ðŸ”¹Keyword Biased Toward: \", keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8PyWWu4IcEe",
        "outputId": "e564cb56-bf0b-44ce-af6b-c1c67eedeffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹Original Title:  Tesla asked to give 1 cr to used car buyer for hiding damage in China\n",
            "ðŸ”¹SEO-Biased Title:  Tesla asked to give 1 cr to used car buyer for hiding damage in China\n",
            "ðŸ”¹Keyword Biased Toward:  Chinese\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi token Logits Processor  \n",
        "Conclusion: not working"
      ],
      "metadata": {
        "id": "5TUgBHuLgNH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiKeywordBiasLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, tokenizer, keyword_weights: Dict[str, float]):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_bias_map = {}\n",
        "\n",
        "        for word, boost in keyword_weights.items():\n",
        "            token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
        "            if len(token_ids) == 1:\n",
        "                self.token_bias_map[token_ids[0]] = boost\n",
        "            else:\n",
        "                print(f\"Skipping multi-token phrase '{word}'. Use phrase biasing instead.\")\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        for token_id, boost in self.token_bias_map.items():\n",
        "            scores[:, token_id] += boost\n",
        "        return scores"
      ],
      "metadata": {
        "id": "3lXYp38YgSMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = {\n",
        "    \"Chinese\": 5.0,\n",
        "    \"damage\": 4.0,\n",
        "    \"court\": 3.0\n",
        "}\n",
        "\n",
        "logits_processor = MultiKeywordBiasLogitsProcessor(tokenizer, keywords)\n",
        "\n",
        "generated = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    logits_processor=[logits_processor]\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAqlRQGTgVNd",
        "outputId": "415157a1-c2bf-4cd4-c245-61027fec1680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla ordered to pay 1 cr to used car buyer for concealing damage to car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase Biasing Logits Processor  \n",
        "Conclusion: Not working"
      ],
      "metadata": {
        "id": "I35Y_D3SewYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PhraseBiasingLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, tokenizer, phrase_bias_map: dict, boost=5.0):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.phrase_bias_map = {}\n",
        "        self.boost = boost\n",
        "\n",
        "        # Convert each phrase to list of token IDs\n",
        "        for phrase, score in phrase_bias_map.items():\n",
        "            token_ids = tokenizer.encode(phrase, add_special_tokens=False)\n",
        "            if len(token_ids) > 1:\n",
        "                self.phrase_bias_map[tuple(token_ids[:-1])] = (token_ids[-1], score)\n",
        "            else:\n",
        "                print(f\"Skipping single-token phrase: '{phrase}'\")\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        # Get last sequence (assumes batch size = 1)\n",
        "        input_ids = input_ids[0].tolist()\n",
        "\n",
        "        for prefix, (next_token, bias_score) in self.phrase_bias_map.items():\n",
        "            if tuple(input_ids[-len(prefix):]) == prefix:\n",
        "                scores[:, next_token] += bias_score\n",
        "        return scores"
      ],
      "metadata": {
        "id": "z_7LY3_je1T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_bias_map = {\n",
        "    \"Chinese court\": 6.0,\n",
        "    \"used car\": 5.0,\n",
        "    \"vehicle damage\": 4.0\n",
        "}\n",
        "\n",
        "logits_processor = PhraseBiasingLogitsProcessor(tokenizer, phrase_bias_map)\n",
        "\n",
        "generated = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_k=50,\n",
        "    logits_processor=[logits_processor]\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcfs4OiBe7C2",
        "outputId": "b9606c81-1f26-47ed-98c8-1892936f7007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla ordered to pay 1 cr to used car buyer for hiding damage in China\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Phrase Biasing with Custom LogitsProcessor  \n",
        "Conclusion: not working"
      ],
      "metadata": {
        "id": "vOn13LJXaVqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PhraseBiasingLogitsProcessor(LogitsProcessor):\n",
        "    def __init__(self, tokenizer, phrase_bias_map: Dict[str, float]):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.phrase_token_ids = {\n",
        "            tuple(tokenizer.encode(phrase, add_special_tokens=False)): boost\n",
        "            for phrase, boost in phrase_bias_map.items()\n",
        "        }\n",
        "\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        for phrase_ids, boost in self.phrase_token_ids.items():\n",
        "            seq_len = len(phrase_ids)\n",
        "            if seq_len == 0 or input_ids.size(1) < seq_len - 1:\n",
        "                continue\n",
        "\n",
        "            # Compare previous (seq_len - 1) tokens\n",
        "            if tuple(input_ids[0, -seq_len + 1:].tolist()) == phrase_ids[:-1]:\n",
        "                next_token_id = phrase_ids[-1]\n",
        "                scores[:, next_token_id] += boost\n",
        "        return scores\n"
      ],
      "metadata": {
        "id": "e5d1eY9yaoHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_bias_map = {\n",
        "    \"Chinese court\": 8.0,\n",
        "    \"structural damage\": 6.0\n",
        "}\n",
        "\n",
        "seo_processor = PhraseBiasingLogitsProcessor(tokenizer, phrase_bias_map)"
      ],
      "metadata": {
        "id": "mqJu4qtkcUeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Generate WITH SEO biasing ===\n",
        "output_ids_seo = model.generate(\n",
        "    input_ids,\n",
        "    max_length=20,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    temperature=0.1,\n",
        "    logits_processor=[seo_processor]\n",
        ")\n",
        "title_seo = tokenizer.decode(output_ids_seo[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "d2GhfScPcj_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 7. Show Results ===\n",
        "print(\"ðŸ”¹ Without SEO Biasing:\", title_plain)\n",
        "print(\"ðŸ”¹ With SEO Biasing   :\", title_seo)\n",
        "print(\"ðŸ”¹ Keyword Biased Toward:\", keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xmg01Rrc9hQ",
        "outputId": "71600648-2f28-4b25-947a-42520731577f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Without SEO Biasing: Tesla to pay 1 cr to Used Car Buyer by Chinese court over damage in S\n",
            "ðŸ”¹ With SEO Biasing   : Tesla ordered to pay 1 cr to used car buyer for concealing damage in China\n",
            "ðŸ”¹ Keyword Biased Toward: structural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard Constrained Decoding  \n",
        "\n",
        "Conclusion: Results are non optimal."
      ],
      "metadata": {
        "id": "2BNJKaob-LRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your required keywords\n",
        "keywords = [\"Chinese\"]  # words you want to appear in output\n",
        "\n",
        "# Convert keywords to token IDs (flattened to handle subwords)\n",
        "required_token_ids = set()\n",
        "for word in keywords:\n",
        "    tokens = tokenizer(word, add_special_tokens=False).input_ids\n",
        "    required_token_ids.update(tokens)\n",
        "\n",
        "# Track which required tokens have been seen\n",
        "seen_token_ids = set()\n",
        "\n",
        "# Define the constraint function\n",
        "def prefix_allowed_tokens_fn(batch_id, input_ids):\n",
        "    global seen_token_ids\n",
        "    # If all required tokens seen, allow full vocab\n",
        "    if required_token_ids.issubset(seen_token_ids):\n",
        "        return list(tokenizer.get_vocab().values())\n",
        "\n",
        "    # Otherwise, only allow required tokens or frequently likely tokens\n",
        "    last_token = input_ids[-1].item()\n",
        "    seen_token_ids.add(last_token)\n",
        "\n",
        "    # Prioritize required tokens to get them included early\n",
        "    return list(required_token_ids.union(set(torch.topk(model.lm_head.weight[last_token], 50).indices.tolist())))\n",
        "\n",
        "# Generate with constrained decoding\n",
        "output_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=30,\n",
        "    num_beams=5,\n",
        "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# Decode and print\n",
        "output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated:\", output)\n"
      ],
      "metadata": {
        "id": "fUDW_qScLzcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a125f97-4582-4594-d1de-d2c3e0753889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: 1 cr ordered to pay Tesla to used car buyer for hiding damage in China\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hot_9hN2_DHl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}